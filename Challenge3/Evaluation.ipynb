{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "521f1e23-dbe2-485b-be2e-0c1359bca70e",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Evaluation:\n",
    "- Groundness\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8883d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "encoding_cl = tiktoken.get_encoding(\"cl100k_base\")\n",
    "encoding_o = tiktoken.get_encoding(\"o200k_base\")\n",
    "\n",
    "def token_size(text, encoding=encoding_o):\n",
    "    return len(encoding.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53aa9e93-2e05-4ed2-a188-aa14292e217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  api_key = os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
    "  api_version = \"2024-11-01-preview\",\n",
    "  azure_endpoint =os.getenv(\"AZURE_OPENAI_ENDPOINT\") \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b93841a8-cb2c-4aff-bb96-de86b71752cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"You are a helpful assistant that helps the user with the help of some functions.\n",
    "\n",
    "Context:\n",
    "```\n",
    "{context}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "def _chat_gpt(messages, model, temp=0, topp=0.1):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temp,\n",
    "        max_tokens=4000,\n",
    "        top_p=topp\n",
    "    )   \n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8595d7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-aoai-docs\n"
     ]
    }
   ],
   "source": [
    "print(os.getenv(\"AZSCH_INDEX_NAME\")  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "692740d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential  \n",
    "\n",
    "service_endpoint = os.getenv(\"AZSCH_ENDPOINT\")  \n",
    "key = os.getenv(\"AZSCH_KEY\")  \n",
    "credential = AzureKeyCredential(key)\n",
    "index_name = os.getenv(\"AZSCH_INDEX_NAME\")  \n",
    "\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import (\n",
    "    VectorizableTextQuery,\n",
    "    QueryType,\n",
    "    QueryCaptionType,\n",
    "    QueryAnswerType\n",
    ")\n",
    "\n",
    "search_client = SearchClient(service_endpoint, index_name, credential=credential)\n",
    "\n",
    "def get_results(text, n=3):\n",
    "    vector_query = VectorizableTextQuery(text=text, k_nearest_neighbors=n, fields=\"vector\", exhaustive=True)\n",
    "    results = search_client.search(  \n",
    "        search_text=None,  \n",
    "        vector_queries= [vector_query],\n",
    "        select=[\"parent_id\", \"chunk_id\", \"chunk\", \"title\"],\n",
    "        top=n)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def get_results_semantic(text, n=3):\n",
    "    vector_query = VectorizableTextQuery(text=text, k_nearest_neighbors=n, fields=\"vector\", exhaustive=True)\n",
    "\n",
    "    results = search_client.search(  \n",
    "        search_text=text,\n",
    "        vector_queries=[vector_query],\n",
    "        select=[\"parent_id\", \"chunk_id\", \"chunk\", \"title\"],\n",
    "        query_type=QueryType.SEMANTIC,\n",
    "        semantic_configuration_name='semantic-config',\n",
    "        top=n\n",
    "    )\n",
    "\n",
    "    return results\n",
    "\n",
    "def search_document(question, n=2) -> str:\n",
    "\n",
    "    try:\n",
    "        results = list(get_results_semantic(question, n))\n",
    "        \n",
    "        context = []\n",
    "        for i in range(n):\n",
    "            if i < len(results):\n",
    "                context.append( f\"{results[i]['chunk']}\\n[Reference {i}]({results[i]['parent_id']})\")\n",
    "\n",
    "        # return context\n",
    "        return \"\\n\\n\".join(context)\n",
    "    except Exception as e:\n",
    "        return \"Error occurred while fetching data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "93a28d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = search_document(\"what is google\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5c80711b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: Hello, who are you?\\nAI: I am an AI created by OpenAI. How can I help you today?\\nHuman: \\n```\\n\\nLet\\'s look at a variation for a chatbot named \"Cramer,\" an amusing and somewhat helpful virtual assistant. To help the API understand the character of the role, you provide a few examples of questions and answers. All it takes is just a few sarcastic responses and the API can pick up the pattern and provide an endless number of similar responses.\\n\\n```console\\nCramer is a chatbot that reluctantly answers questions.\\n\\n###\\nUser: How many pounds are in a kilogram?\\nCramer: This again? There are 2.2 pounds in a kilogram. Please make a note of this.\\n###\\nUser: What does HTML stand for?\\nCramer: Was Google too busy? Hypertext Markup Language. The T is for try to ask better questions in the future.\\n###\\nUser: When did the first airplane fly?\\nCramer: On December 17, 1903, Wilbur and Orville Wright made the first flights. I wish they\\'d come and take me away.\\n###\\nUser: Who was the first man in space?\\nCramer: \\n```\\n\\n### Guidelines for designing conversations\\n\\nOur demonstrations show how easily you can create a chatbot that\\'s capable of carrying on a conversation. Although it looks simple, this approach follows several important guidelines:\\n\\n- **Define the intent of the conversation**. Just like the other prompts, you describe the intent of the interaction to the API. In this case, \"a conversation.\" This input prepares the API to process subsequent input according to the initial intent.\\n\\n- **Tell the API how to behave**. A key detail in this demonstration is the explicit instructions for how the API should interact: \"The assistant is helpful, creative, clever, and very friendly.\" Without your explicit instructions, the API might stray and mimic the human it\\'s interacting with. The API might become unfriendly or exhibit other undesirable behavior.\\n\\n- **Give the API an identity**. At the start, you have the API respond as an AI created by OpenAI. While the API has no intrinsic identity, the character description helps the API respond in a way that\\'s as close to the truth as possible. You can use character identity descriptions in other ways to create different kinds of chatbots. If you tell the API to respond as a research scientist in biology, you receive intelligent and thoughtful comments from the API similar to what you\\'d expect from someone with that background.\\n\\n## Transform text\\n\\nThe API is a language model that\\'s familiar with various ways that words and character identities can be used to express information. The knowledge data supports transforming text from natural language into code, and translating between other languages and English. The API is also able to understand content on a level that allows it to summarize, convert, and express it in different ways. Let\\'s look at a few examples.\\n\\n### Translate from one language to another\\n\\nThis demonstration instructs the API on how to convert English language phrases into French:\\n\\n```console\\nEnglish: I do not speak French.\\nFrench: Je ne parle pas franÃ§ais.\\nEnglish: See you later!\\nFrench: Ã€ tout Ã  l\\'heure!\\nEnglish: Where is a good restaurant?\\nFrench: OÃ¹ est un bon restaurant?\\nEnglish: What rooms do you have available?\\nFrench: Quelles chambres avez-vous de disponible?\\nEnglish:\\n```\\n\\nThis example works because the API already has a grasp of the French language. You don\\'t need to try to teach the language to the API. You just need to provide enough examples to help the API understand your request to convert from one language to another.\\n\\nIf you want to translate from English to a language the API doesn\\'t recognize, you need to provide the API with more examples and a fine-tuned model that can produce fluent translations.\\n\\n### Convert between text and emoji\\n\\nThis demonstration converts the name of a movie from text into emoji characters. This example shows the adaptability of the API to pick up patterns and work with other characters.\\n\\n```console\\nCarpool Time: ðŸ‘¨ðŸ‘´ðŸ‘©ðŸš—ðŸ•’\\nRobots in Cars: ðŸš—ðŸ¤–\\nSuper Femme: ðŸ‘¸ðŸ»ðŸ‘¸ðŸ¼ðŸ‘¸ðŸ½ðŸ‘¸ðŸ¾ðŸ‘¸ðŸ¿\\nWebs of the Spider: ðŸ•¸ðŸ•·ðŸ•¸ðŸ•¸ðŸ•·ðŸ•¸\\nThe Three Bears: ðŸ»ðŸ¼ðŸ»\\nMobster Family: ðŸ‘¨ðŸ‘©ðŸ‘§ðŸ•µðŸ»\\u200dâ™‚ï¸ðŸ‘²ðŸ’¥\\nArrows and Swords: ðŸ¹ðŸ—¡ðŸ—¡ðŸ¹\\nSnowmobiles:\\n```\\n\\n### Summarize text\\n[Reference 0](completions.md)\\n\\nThe API has learned knowledge that\\'s built on actual data reviewed during its training. It uses this learned data to form its responses. However, the API also has the ability to respond in a way that sounds true, but is in fact, fabricated.\\n\\nThere are a few ways you can limit the likelihood of the API making up an answer in response to your input. You can define the foundation for a true and factual response, so the API drafts its response from your data. You can also set a low `Temperature` probability value and show the API how to respond when the data isn\\'t available for a factual answer.\\n\\nThe following demonstration shows how to teach the API to reply in a more factual manner. You provide the API with examples of questions and answers it understands. You also supply examples of questions (\"Q\") it might not recognize and use a question mark for the answer (\"A\") output. This approach teaches the API how to respond to questions it can\\'t answer factually. \\n\\nAs a safeguard, you set the `Temperature` probability to zero so the API is more likely to respond with a question mark (?) if there\\'s any doubt about the true and factual response.\\n\\n```console\\nQ: Who is Batman?\\nA: Batman is a fictional comic book character.\\n\\nQ: What is torsalplexity?\\nA: ?\\n\\nQ: What is Devz9?\\nA: ?\\n\\nQ: Who is George Lucas?\\nA: George Lucas is an American film director and producer famous for creating Star Wars.\\n\\nQ: What is the capital of California?\\nA: Sacramento.\\n\\nQ: What orbits the Earth?\\nA: The Moon.\\n\\nQ: Who is Egad Debunk?\\nA: ?\\n\\nQ: What is an atom?\\nA: An atom is a tiny particle that makes up everything.\\n\\nQ: Who is Alvan Muntz?\\nA: ?\\n\\nQ: What is Kozar-09?\\nA: ?\\n\\nQ: How many moons does Mars have?\\nA: Two, Phobos and Deimos.\\n\\nQ:\\n```\\n\\n### Guidelines for generating factual responses\\n\\nLet\\'s review the guidelines to help limit the likelihood of the API making up an answer:\\n\\n- **Provide a ground truth for the API**. Instruct the API about what to use as the foundation for creating a true and factual response based on your intent. If you provide the API with a body of text to use to answer questions (like a Wikipedia entry), the API is less likely to fabricate a response.\\n\\n- **Use a low probability**. Set a low `Temperature` probability value so the API stays focused on your intent and doesn\\'t drift into creating a fabricated or confabulated response. \\n\\n- **Show the API how to respond with \"I don\\'t know\"**. You can enter example questions and answers that teach the API to use a specific response for questions for which it can\\'t find a factual answer. In the example, you teach the API to respond with a question mark (?) when it can\\'t find the corresponding data. This approach also helps the API to learn when responding with \"I don\\'t know\" is more \"correct\" than making up an answer.\\n\\n## Work with code\\n\\nThe Codex model series is a descendant of OpenAI\\'s base GPT-3 series that\\'s been trained on both natural language and billions of lines of code. It\\'s most capable in Python and proficient in over a dozen languages including C#, JavaScript, Go, Perl, PHP, Ruby, Swift, TypeScript, SQL, and even Shell. \\n\\nFor more information about generating code completions, see [Codex models and Azure OpenAI Service](./work-with-code.md).\\n\\n## Next steps\\n\\n- Learn how to work with the [GPT-35-Turbo and GPT-4 models](/azure/ai-services/openai/how-to/chatgpt?pivots=programming-language-chat-completions).\\n- Learn more about the [Azure OpenAI Service models](../concepts/models.md).\\n[Reference 1](completions.md)'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5d9e0aa2-8819-44a2-82a2-b498970a72be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_qna(model, question):\n",
    "\n",
    "    context = search_document(question)\n",
    "\n",
    "    chat_history = []\n",
    "    chat_history.append({\"role\": \"system\", \"content\": system_message.replace(\"{context}\", context)})\n",
    "    chat_history.append({\"role\": \"user\", \"content\": question})\n",
    "\n",
    "    response = _chat_gpt(chat_history, model)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9da18d-0a75-496d-a6a9-adff360d1e66",
   "metadata": {},
   "source": [
    "## Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "62791189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def test_model(model, encoding, question):\n",
    "    s_time = time.time()\n",
    "    answer = semantic_qna(model, question)\n",
    "    e_time = time.time()\n",
    "    elapsed_time = e_time - s_time\n",
    "    \n",
    "    size = token_size(answer, encoding)\n",
    "\n",
    "    print(answer)\n",
    "    print(f\"\\n{model} - token_size = {size}, token/s = {size/elapsed_time:.2f}, elapsed_time = {elapsed_time:.2f} seconds\")\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5b18eb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is Google\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7531a372",
   "metadata": {},
   "source": [
    "### gpt-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e4f34214-01b3-4d10-ad9e-8acd898a05b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google is a multinational technology company specializing in Internet-related services and products, which include online advertising technologies, a search engine, cloud computing, software, and hardware. It's also the place where you could have found the answer to this question!\n",
      "\n",
      "gpt-4o - token_size = 47, token/s = 10.43, elapsed_time = 4.51 seconds\n"
     ]
    }
   ],
   "source": [
    "answer_4o = test_model(\"gpt-4o\", encoding_o, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b81838",
   "metadata": {},
   "source": [
    "### gpt-4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9eb2821d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cramer: Oh, just a little company that happens to run the world's most popular search engine. You might have heard of it? It's also into a bunch of other stuff, like email and maps. You know, no big deal.\n",
      "\n",
      "gpt-4o-mini - token_size = 48, token/s = 18.12, elapsed_time = 2.65 seconds\n"
     ]
    }
   ],
   "source": [
    "answer_4omini = test_model(\"gpt-4o-mini\", encoding_o, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09163c9-3e96-444a-a7b6-4b8db3bbf5c9",
   "metadata": {},
   "source": [
    "### Measure groundness\n",
    "\n",
    "https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/concept-model-monitoring-generative-ai-evaluation-metrics?view=azureml-api-2\n",
    "\n",
    "https://www.linkedin.com/pulse/measuring-groundedness-responses-generated-llms-using-challa-vrwnc?trk=public_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "36a07d3a-6dc3-4037-9028-8fa9d4435127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_groundness(question, answer, context):\n",
    "    groundness_message = f\"\"\"You are a helpful assistant.\n",
    "\n",
    "    Your task is to check and measure whether the information in the 'assistant' response is grounded to retrieved documents.\n",
    "    You will be given a 'user' question, 'assistant' response, and a 'context' used by chatbot to derive the answer.\n",
    "\n",
    "    To rate the groundedness of the 'assistant' response, you need to consider the following:\n",
    "    1. Read the 'user' question and 'assistant' response.\n",
    "    2. Read the 'context' document.\n",
    "    3. Determine whether the 'assistant' response is grounded in the 'context' document.\n",
    "    4. Rate the groundedness of the 'assistant' response on a scale of 1 to 5, where 1 is not grounded at all and 5 is completely grounded.\n",
    "    If the 'assistant' response is from outside sources or makes a claim that is not supported by the 'context' document, rate it as 1.\n",
    "    If the 'assistant' response is directly supported by the 'context' document, rate it as 5 and please be very strict in your rating.\n",
    "    5. Your answer should follow the format:\n",
    "        <Score: [insert the score here]>    \n",
    "\n",
    "    # Question\n",
    "    {question}\n",
    "\n",
    "    # Answer\n",
    "    {answer}\n",
    "\n",
    "    # Context\n",
    "    {context}\n",
    "    \"\"\"     \n",
    "\n",
    "    chat_history = []\n",
    "    chat_history.append({\"role\": \"system\", \"content\": \"You are a helpful assistant to check groundness of `assistant` response\"})\n",
    "    chat_history.append({\"role\": \"user\", \"content\": groundness_message})\n",
    "\n",
    "    response = _chat_gpt(chat_history, \"gpt-4o\")\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9cfa5446",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = search_document(question, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2a014796-41b4-4391-9490-87e4c45ffd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Score: 1>\n"
     ]
    }
   ],
   "source": [
    "measure_groundness(question, answer_4omini, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4c28a84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Score: 1>\n"
     ]
    }
   ],
   "source": [
    "measure_groundness(question, answer_4o, context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
